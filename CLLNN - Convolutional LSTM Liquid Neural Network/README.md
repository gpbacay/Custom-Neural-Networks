# Convolutional LSTM Liquid Neural Network (CLLNN)
###### Bacay, Gianne P. (2024)


![CLLNN Model Architecture Diagram](CLLNN_ArchitectureDesign.drawio.png)

## Table of Contents

1. [Introduction](#introduction)
2. [What is a Convolutional LSTM Liquid Neural Network (CLLNN)?](#what-is-a-convolutional-lstm-liquid-neural-network-cllnn)
3. [Architecture of CLLNN](#architecture-of-cllnn)
4. [How Does CLLNN Work?](#how-does-cllnn-work)
5. [Implementation of CLLNN](#implementation-of-cllnn)
    1. [Import Libraries](#import-libraries)
    2. [Define LNN Reservoir Layer](#define-lnn-reservoir-layer)
    3. [Initialize Weights](#initialize-weights)
    4. [Build CLLNN Model](#build-cllnn-model)
    5. [Load and Preprocess MNIST Data](#load-and-preprocess-mnist-data)
    6. [Set Hyperparameters and Train](#set-hyperparameters-and-train)
6. [Results](#results)
7. [Conclusion](#conclusion)
8. [Advantages and Disadvantages of CLLNN](#advantages-and-disadvantages-of-cllnn)
9. [Applications of CLLNN](#applications-of-cllnn)

## Introduction

The Convolutional LSTM Liquid Neural Network (CLLNN) is a hybrid model combining the strengths of Convolutional Neural Networks (CNNs), Liquid Neural Networks (LNNs), and Long Short-Term Memory (LSTM) networks. This document provides an overview of CLLNN, detailing its architecture, implementation, advantages, and applications, with a specific focus on its use for MNIST classification.

## What is a Convolutional LSTM Liquid Neural Network (CLLNN)?

The Convolutional LSTM Liquid Neural Network (CLLNN) integrates CNNs, Liquid Neural Networks (LNNs), and Long Short-Term Memory (LSTM) networks to leverage convolutional feature extraction, liquid state reservoirs, and sequence learning capabilities.

- **Convolutional Neural Networks (CNNs)**: CNNs are designed to automatically and adaptively learn spatial hierarchies of features from input images.
- **Liquid Neural Networks (LNNs)**: These are a type of Recurrent Neural Network (RNN) with dynamic, time-continuous architectures that adapt to data changes, mimicking the fluidity of liquids.
- **Long Short-Term Memory (LSTM)**: LSTM networks are a specialized type of RNN designed to handle long-term dependencies and sequential data by maintaining a memory cell that can preserve information over long periods.

By combining CNNs' powerful feature extraction, LNNs' adaptive reservoir dynamics, and LSTM's robust sequence learning, the CLLNN aims to enhance performance on complex image classification tasks.

## Architecture of CLLNN

![CLLNN Model Architecture Diagram](CLLNN_ArchitectureDesign.drawio.png)

The architecture of CLLNN consists of the following key components:

1. **Input Layer**:
   - Receives input data and passes it to the convolutional layers.

2. **Convolutional Layers**:
   - Extract spatial features from input images through multiple convolutional and pooling layers.

3. **Liquid Layer (Reservoir)**:
   - Composed of a large recurrent network of neurons.
   - Transforms feature data into a rich, non-linear space using first-order dynamical systems.

4. **LSTM Layer**:
   - Follows the liquid layer.
   - Handles long-term dependencies and sequential information using LSTM cells.

5. **Output Layer**:
   - Produces final predictions based on the processed information from the LSTM layer.

## How Does CLLNN Work?

1. **Feature Extraction**:
   - Convolutional layers extract spatial hierarchies of features from input images.

2. **Reservoir Dynamics**:
   - The liquid layer uses differential equations to model neuron interactions and updates reservoir states over time.
   - Reservoir states capture complex, non-linear features from the input data.

3. **LSTM Processing**:
   - The LSTM layer takes the reservoir outputs as input.
   - It manages long-term dependencies and refines the sequence information using its memory cells.

4. **Prediction**:
   - The output layer aggregates information from the LSTM layer to generate predictions.

## Implementation of CLLNN

Hereâ€™s a step-by-step guide to implementing a CLLNN using TensorFlow for MNIST classification:

### Import Libraries

```python
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Conv2D, MaxPooling2D, LSTM, Dense, Input, Flatten, Reshape
from tensorflow.keras.models import Model
from tensorflow.keras.datasets import mnist
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
```

## Define LNN Reservoir Layer

```python
class LNNStep(tf.keras.layers.Layer):
    def __init__(self, reservoir_weights, input_weights, leak_rate, **kwargs):
        super(LNNStep, self).__init__(**kwargs)
        self.reservoir_weights = tf.constant(reservoir_weights, dtype=tf.float32)
        self.input_weights = tf.constant(input_weights, dtype=tf.float32)
        self.leak_rate = leak_rate
        self.reservoir_dim = reservoir_weights.shape[0]

    @property
    def state_size(self):
        return (self.reservoir_dim,)

    def call(self, inputs, states):
        prev_state = states[0]
        input_part = tf.matmul(inputs, self.input_weights, transpose_b=True)
        reservoir_part = tf.matmul(prev_state, self.reservoir_weights, transpose_b=True)
        state = (1 - self.leak_rate) * prev_state + self.leak_rate * tf.tanh(input_part + reservoir_part)
        return state, [state]

    def get_config(self):
        config = super().get_config().copy()
        config.update({
            "reservoir_weights": self.reservoir_weights.numpy().tolist(),
            "input_weights": self.input_weights.numpy().tolist(),
            "leak_rate": self.leak_rate,
        })
        return config

    @classmethod
    def from_config(cls, config):
        reservoir_weights = np.array(config['reservoir_weights'])
        input_weights = np.array(config['input_weights'])
        leak_rate = config['leak_rate']
        return cls(reservoir_weights, input_weights, leak_rate)
```

## Initialize Weights
```
python
def initialize_lnn_reservoir(input_dim, reservoir_dim, spectral_radius):
    reservoir_weights = np.random.randn(reservoir_dim, reservoir_dim)
    reservoir_weights *= spectral_radius / np.max(np.abs(np.linalg.eigvals(reservoir_weights)))
    input_weights = np.random.randn(reservoir_dim, input_dim) * 0.1
    return reservoir_weights, input_weights
```


## Build CLLNN Model

```python
def create_cllnn_model(input_shape, reservoir_dim, spectral_radius, leak_rate, lstm_units, output_dim):
    inputs = Input(shape=input_shape)

    # CNN Layers
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2))(x)
    
    # Flatten and Reshape for LSTM
    x = Flatten()(x)
    cnn_output_shape = x.shape[1]
    
    # Ensure input_dim for LNN matches the flattened output shape
    input_dim = cnn_output_shape
    x = Reshape((1, cnn_output_shape))(x)  # Reshape for LSTM input

    # Initialize LNN weights
    reservoir_weights, input_weights = initialize_lnn_reservoir(input_dim, reservoir_dim, spectral_radius)

    # LNN Layer
    lnn_layer = tf.keras.layers.RNN(LNNStep(reservoir_weights, input_weights, leak_rate), return_sequences=True)
    lnn_output = lnn_layer(x)

    # LSTM Layer
    lstm_output = LSTM(lstm_units)(lnn_output)

    # Output Layer
    outputs = Dense(output_dim, activation='softmax')(lstm_output)

    model = keras.Model(inputs, outputs)
    return model
```

## Load and Preprocess MNIST Data

```python
# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)

# Normalize the data
def normalize_data(x):
    num_samples, height, width = x.shape
    x = x.reshape(-1, width)
    scaler = StandardScaler()
    x = scaler.fit_transform(x)
    return x.reshape(num_samples, height, width)

x_train = normalize_data(x_train)
x_val = normalize_data(x_val)
x_test = normalize_data(x_test)

# Reshape for CNN input
x_train = x_train.reshape(-1, 28, 28, 1)
x_val = x_val.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

# One-hot encode the labels
y_train = keras.utils.to_categorical(y_train, 10)
y_val = keras.utils.to_categorical(y_val, 10)
y_test = keras.utils.to_categorical(y_test, 10)
```

## Set Hyperparameters and Train

```python
# Hyperparameters
input_shape = (28, 28, 1)
reservoir_dim = 512
spectral_radius = 0.9
leak_rate = 0.2
lstm_units = 128
output_dim = 10

# Create and compile the model
cllnn_model = create_cllnn_model(input_shape, reservoir_dim, spectral_radius, leak_rate, lstm_units, output_dim)
cllnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = cllnn_model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_val, y_val))

# Evaluate the model
test_loss, test_accuracy = cllnn_model.evaluate(x_test, y_test)
print(f"Test accuracy: {test_accuracy:.4f}")
```

## Results and Analysis

```plaintext

2024-08-02 17:23:09.978943: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Epoch 1/10
844/844 [==============================] - 74s 78ms/step - loss: 0.1791 - accuracy: 0.9525 - val_loss: 0.0552 - val_accuracy: 0.9853 - lr: 0.0010
Epoch 2/10
844/844 [==============================] - 63s 74ms/step - loss: 0.0431 - accuracy: 0.9876 - val_loss: 0.0400 - val_accuracy: 0.9868 - lr: 0.0010
Epoch 3/10
844/844 [==============================] - 61s 72ms/step - loss: 0.0293 - accuracy: 0.9913 - val_loss: 0.0385 - val_accuracy: 0.9868 - lr: 0.0010
Epoch 4/10
844/844 [==============================] - 63s 75ms/step - loss: 0.0204 - accuracy: 0.9939 - val_loss: 0.0327 - val_accuracy: 0.9903 - lr: 0.0010
Epoch 5/10
844/844 [==============================] - 64s 76ms/step - loss: 0.0160 - accuracy: 0.9953 - val_loss: 0.0400 - val_accuracy: 0.9873 - lr: 0.0010
Epoch 6/10
Epoch 8/10
844/844 [==============================] - 63s 75ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.0312 - val_accuracy: 0.9910 - lr: 1.0000e-04 
Epoch 9/10
844/844 [==============================] - 60s 71ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.0319 - val_accuracy: 0.9903 - lr: 1.0000e-04 
Epoch 10/10
844/844 [==============================] - 64s 76ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.0313 - val_accuracy: 0.9912 - lr: 1.0000e-05 
313/313 - 5s - loss: 0.0240 - accuracy: 0.9931 - 5s/epoch - 18ms/step
Test Accuracy: 0.9931
```

After training the CLLNN model on the MNIST dataset for 10 epochs, we achieved a test accuracy of 0.9931, indicating that the model performed exceptionally well on handwritten digit classification.

**Analysis:**

1. **Training and Validation Loss**:
    
    - The model demonstrates a significant reduction in training loss from 0.1791 in the first epoch to 0.0011 by the tenth epoch. This indicates effective learning and convergence.
    - Validation loss decreases consistently, reflecting that the model generalizes well to unseen data, maintaining low error rates.
2. **Training and Validation Accuracy**:
    
    - Training accuracy increases from 95.25% in the first epoch to 99.99% by the end of training, showing robust learning of MNIST digit classification.
    - Validation accuracy shows a similar upward trend, from 98.53% to 99.12%, indicating that the model performs well on validation data and is not overfitting.
3. **Test Accuracy**:
    
    - The final test accuracy of 99.31% confirms the model's exceptional performance on unseen test data, suggesting that the CLLNN architecture effectively captures and generalizes digit patterns in MNIST images.
4. **Learning Rate Adjustment**:
    
    - Observing the learning rate adjustments, the decrease towards the end of training (from 0.001 to 1e-05) helps the model fine-tune its parameters more precisely as it converges.
5. **Model Behavior**:
    
    - The sharp decline in loss and steady increase in accuracy throughout the epochs illustrate the efficacy of the CLLNN model in both training and generalization.

In summary, the results validate the effectiveness of the Convolutional LSTM Liquid Neural Network (CLLNN) in handling complex image classification tasks. The model's high accuracy on the MNIST dataset, coupled with its ability to maintain performance across training and validation sets, underscores its potential for other sophisticated image and sequence learning applications.

## Conclusion

The Convolutional LSTM Liquid Neural Network (CLLNN) demonstrates a successful integration of CNNs, LNNs, and LSTM networks, leveraging their strengths to achieve high performance on image classification tasks. The adaptive and dynamic nature of LNNs, combined with the robust feature extraction capabilities of CNNs and the sequence learning potential of LSTM networks, makes CLLNN a promising architecture for various applications.

## Advantages and Disadvantages of CLLNN
#### Advantages

1. **Robust Feature Extraction**: CNN layers effectively capture spatial features from input images.
2. **Adaptive Dynamics**: The liquid layer's reservoir adapts to changes in data, providing dynamic feature transformations.
3. **Long-term Dependency Handling**: LSTM layers manage sequential dependencies and maintain long-term information.
4. **High Performance**: CLLNN achieves high accuracy on complex image classification tasks like MNIST.

#### Disadvantages

1. **Computational Complexity**: The integration of CNNs, LNNs, and LSTM layers increases computational requirements.
2. **Resource Intensive**: Training and inference can be resource-intensive due to the complexity of the architecture.
3. **Parameter Tuning**: Requires careful tuning of hyperparameters to achieve optimal performance.

## Applications of CLLNN

CLLNNs are versatile and well-suited for various applications, particularly those requiring sophisticated feature extraction and sequence learning:

- **MNIST Classification**: Demonstrated high accuracy in classifying handwritten digits, showcasing the model's strong performance in image classification tasks.
- **Time-Series Analysis**: Effective for analyzing and predicting patterns in temporal data, such as financial market trends or sensor readings.
- **Speech Recognition**: Enhances the accuracy of transcribing spoken language by leveraging its ability to handle complex sequential information.
- **Autonomous Systems**: Improves decision-making processes in autonomous vehicles, drones, or robotics by integrating feature extraction with dynamic sequence learning.

This documentation provides a comprehensive overview of the Convolutional LSTM Liquid Neural Network (CLLNN), detailing its architecture, implementation, and applications. The CLLNN combines the strengths of convolutional feature extraction, adaptive reservoir dynamics, and robust sequence learning, making it a powerful tool for a variety of tasks involving complex image and sequence data.
